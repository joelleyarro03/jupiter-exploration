{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joelleyarro03/jupiter-exploration/blob/main/workshop_1_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "IDvUFfXYajLT"
      },
      "source": [
        "# I.  Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWOCLSYqajLW"
      },
      "source": [
        "<center><img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/preview.jpg?raw=1\" width=\"700\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "pXu57RR0ajLX"
      },
      "source": [
        "In this project, we'll build a neural network classifier that determines: **MUFFIN... or CHIHUAHUA!**\n",
        "\n",
        "This is what we'll cover in the tutorial:\n",
        "#### 1) Build the neural network\n",
        "#### 2) Load the data\n",
        "#### 3) Train the model on the data\n",
        "#### 4) Visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlSLRWYnajLX"
      },
      "source": [
        "### Remember: This is an INTERACTIVE Notebook!\n",
        "You should run and play with the code as you go to see how it works. Select a cell and **press shift-enter to execute code.**\n",
        "\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/jupyter_animated.gif?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "Qj5475ELajLY"
      },
      "source": [
        "# II.  Deep Learning Tutorial\n",
        "\n",
        "Let's get to the fun stuff!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWRpOUDwajLY"
      },
      "source": [
        "<center><img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/Pytorch_logo.png?raw=1\" width=\"700\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdUxlcL2ajLY"
      },
      "source": [
        "**Generic Python imports** (select the below cell and press shift-enter to execute it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3bZ32VVpajLZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # graphical library, to plot images\n",
        "# special Jupyter notebook command to show plots inline instead of in a new window\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "oag8L1DTajLa"
      },
      "source": [
        "**Deep learning imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "W8XTF1I3ajLb"
      },
      "outputs": [],
      "source": [
        "import torch                                            # PyTorch deep learning framework\n",
        "from torchvision import datasets, models, transforms    # extension to PyTorch for dataset management\n",
        "import torch.nn as nn                                   # neural networks module of PyTorch, to let us define neural network layers\n",
        "from torch.nn import functional as F                    # special functions\n",
        "import torch.optim as optim                             # optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "fmxqWbTIajLb"
      },
      "source": [
        "## (1) Build our Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHlhuBjqajLb"
      },
      "source": [
        "Recall from the lesson that a neural network generally looks like this. Input is on the left, output is on the right. The number of output neurons correspond to the number of classes.\n",
        "\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/what_is_nn_slide.jpg?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0IU6P5DajLb"
      },
      "source": [
        "So let's define a similar architecture for our 2-class muffin-vs-chihuahua classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DwJcQylwajLc"
      },
      "outputs": [],
      "source": [
        "# Extends PyTorch's neural network baseclass\n",
        "class MySkynet(nn.Module):\n",
        "    \"\"\"\n",
        "    A very basic neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(3, 224, 224)):\n",
        "        \"\"\"\n",
        "        Constructs a neural network.\n",
        "\n",
        "        input_dim: a tuple that represents \"channel x height x width\" dimensions of the input\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # the total number of RGB pixels in an image is the tensor's volume\n",
        "        num_in_features = input_dim[0] * input_dim[1] * input_dim[2]\n",
        "        # input layer\n",
        "        self.layer_0 = nn.Linear(num_in_features, 128)\n",
        "        # hidden layers\n",
        "        self.layer_1 = nn.Linear(128, 64)\n",
        "        self.layer_2= nn.Linear(64, 32)\n",
        "        # output layer, output size of 2 for chihuahua and muffin\n",
        "        self.layer_3= nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass through our network.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        # convert our RGB tensor into one long vector\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # pass through our layers\n",
        "        x = F.relu(self.layer_0(x))\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = F.relu(self.layer_3(x))\n",
        "\n",
        "        # convert the raw output to probability predictions\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKA7Z23_ajLc"
      },
      "source": [
        "Now that we've defined the network above, let's initialize it. If available, we'll place the network on the GPU; if not, it goes on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "UtwOiPLcajLc",
        "outputId": "2f851a6b-5206-4894-b557-3061f7744a21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MySkynet(\n",
              "  (layer_0): Linear(in_features=150528, out_features=128, bias=True)\n",
              "  (layer_1): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (layer_2): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (layer_3): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        " # cuda:0 means the first cuda device found\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MySkynet().to(device)                      # load our simple neural network\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtoeogABajLd"
      },
      "source": [
        "Essentially, our network looks like this:\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/architecture.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "LScOkEPdajLd"
      },
      "source": [
        "## (2) Data and Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we74dIBFajLd"
      },
      "source": [
        "### Separate \"train\" and \"test\" datasets\n",
        "\n",
        "Recall from the below slide, we should make two separate datasets to train and test our model. That way, we know our model learns more than rote memorization.\n",
        "\n",
        "<center><img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/when_is_your_model_ready_slide.jpg?raw=1\" width=\"600\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbTsUaMFajLd"
      },
      "source": [
        "### Inspect our data\n",
        "Let's look in our data folder to see what's there. As you can see, the folder is **split into \"train\" for training**, and **\"validation\" for testing** (to validate our model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Q70Y7QctajLd",
        "outputId": "a53e361f-05e4-4d19-b597-1d5831d98ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TnSJea-ajLe"
      },
      "source": [
        "Let's also look at some of the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "id": "LXVkExK7ajLe",
        "outputId": "f1cebf91-521b-41c5-f3bd-3ecb5935ab47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image data/train/chihuahua/4.jpg: [Errno 2] No such file or directory: '/content/data/train/chihuahua/4.jpg'\n",
            "Error loading image data/train/chihuahua/5.jpg: [Errno 2] No such file or directory: '/content/data/train/chihuahua/5.jpg'\n",
            "Error loading image data/train/muffin/131.jpg: [Errno 2] No such file or directory: '/content/data/train/muffin/131.jpg'\n",
            "Error loading image data/train/muffin/107.jpg: [Errno 2] No such file or directory: '/content/data/train/muffin/107.jpg'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Set up the figure to display images\n",
        "_, ax = plt.subplots(1, 4, figsize=(15, 5))  # 1 row x 4 columns\n",
        "\n",
        "# List of image paths\n",
        "image_paths = [\n",
        "    \"data/train/chihuahua/4.jpg\",\n",
        "    \"data/train/chihuahua/5.jpg\",\n",
        "    \"data/train/muffin/131.jpg\",\n",
        "    \"data/train/muffin/107.jpg\"\n",
        "]\n",
        "\n",
        "for i, path in enumerate(image_paths):\n",
        "    try:\n",
        "        img = Image.open(path)  # Load the image\n",
        "        ax[i].imshow(img)  # Show the image\n",
        "        ax[i].axis('off')  # Hide axes\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {path}: {e}\")\n",
        "\n",
        "# Display the images\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "wNFBcdxKajLe"
      },
      "source": [
        "If you look in the data folder on your computer, there are 120 train images and 30 validation. So our data is split like this:\n",
        "\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/folders.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sh68XLNajLe"
      },
      "source": [
        "### Load our data\n",
        "\n",
        "That's great that we have data! But we have to load all the images and convert them into a form that our neural network understands. Specifically, PyTorch works with **Tensor** objects. (A tensor is just a multidimensional matrix, i.e. an N-d array.)\n",
        "\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/image_to_tensor.jpg?raw=1\" width=\"550\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pBNGO-yajLe"
      },
      "source": [
        "**To easily convert our image data into tensors, we use the help of a \"dataloader.\"** The dataloader packages data into convenient boxes for our model to use. You can think of it like one person passing boxes (tensors) to another.\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/dataloader_box_analogy.jpg?raw=1\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "VftZyAYlajLf"
      },
      "source": [
        "**First, we define some \"transforms\" to convert images to tensors.** We must do so for both our train and validation datasets.\n",
        "\n",
        "For more information about transforms, check out the link here: https://pytorch.org/docs/stable/torchvision/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "b_JifN1cajLf",
        "outputId": "26f228cf-d935-4615-9117-cdb0133d72cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train transforms: Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                 std=[0.5, 0.5, 0.5])\n",
        "\n",
        "# transforms for our training data\n",
        "train_transforms = transforms.Compose([\n",
        "    # resize to resnet input size\n",
        "    transforms.Resize((224,224)),\n",
        "    # transform image to PyTorch tensor object\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# these validation transforms are exactly the same as our train transforms\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "print(\"Train transforms:\", train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L509BfOrajLf"
      },
      "source": [
        "**Second, we create the datasets, by passing the transforms into the ImageFolder constructor.**\n",
        "\n",
        "These just represent the folders that hold the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "pudfXGSUajLf"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # Add any other transformations you need\n",
        "])\n",
        "\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # Add any other transformations you need\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ0iYOKZajLf"
      },
      "source": [
        "**And finally, form dataloaders from the datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Create the datasets\n",
        "# try:\n",
        "#     image_datasets = {\n",
        "#         'train': datasets.ImageFolder(train_dir, train_transforms),\n",
        "#         'validation': datasets.ImageFolder(validation_dir, validation_transforms)\n",
        "#     }\n",
        "#     print(\"Datasets created successfully.\")\n",
        "# except Exception as e:\n",
        "#     print(\"Error creating datasets:\", e)\n",
        "# # Create the DataLoaders\n",
        "# try:\n",
        "#     dataloaders = {\n",
        "#         'train': torch.utils.data.D\n",
        "\n",
        "train_dir = 'data/train'\n",
        "validation_dir = 'data/validation'\n",
        "\n",
        "# Create the datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "njH-e2phtKvR"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "U-nQthv6ajLf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define paths to your dataset\n",
        "data_dir = 'path/to/your/data'  # Replace with your dataset path\n",
        "\n",
        "# Define transforms for the training and validation sets\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create datasets\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms['train']),\n",
        "    'validation': datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=data_transforms['validation']),\n",
        "}\n",
        "\n",
        "# Create the DataLoaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(\n",
        "        image_datasets['train'],\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    ),\n",
        "    'validation': DataLoader(\n",
        "        image_datasets['validation'],\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "}\n",
        "\n",
        "# Print DataLoader information\n",
        "print(\"Train loader:\", dataloaders[\"train\"])\n",
        "print(\"Validation loader:\", dataloaders[\"validation\"])\n",
        "\n",
        "# Example to iterate through the train DataLoader\n",
        "for images, labels in dataloaders[\"train\"]:\n",
        "    print(\"Batch of images shape:\", images.size())\n",
        "    print(\"Batch of labels shape:\", labels.size())\n",
        "    break  # Just show the first batch\n",
        " Create datasets\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms['train']),\n",
        "    'validation': datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=data_transforms['validation']),\n",
        "}\n",
        "\n",
        "# Create the DataLoaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(\n",
        "        image_datasets['train'],\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    ),\n",
        "    'validation': DataLoader(\n",
        "        image_datasets['validation'],\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUL6t0p3ajLg"
      },
      "source": [
        "We can see a dataloader outputs 2 things: a BIG tensor to represent an image, and a vector to represent the labels (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "hW6nE3ADajLg"
      },
      "outputs": [],
      "source": [
        "# Get the first batch from the train DataLoader\n",
        "images, labels = next(iter(dataloaders[\"train\"]))\n",
        "\n",
        "# Print shapes and a sample of the outputs\n",
        "print(\"Batch of images shape:\", images.size())  # e.g., torch.Size([8, 3, 224, 224])\n",
        "print(\"Batch of labels shape:\", labels.size())  # e.g., torch.Size([8])\n",
        "print(\"Sample labels:\", labels)  # Example output: tensor([0, 1, 0, 1, 1, 0, 0, 1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "KZ6pmlNkajLg"
      },
      "source": [
        "## (4) Train the model!\n",
        "\n",
        "Hurray! We've built a neural network and have data to give it. Now we **repeatedly iterate over the data to train the model.**\n",
        "\n",
        "Every time the network gets a new example, it looks something like this. Note the **forward pass** and the corresponding **backward pass**.\n",
        "\n",
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/backpropagation.gif?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOc12nnvajLg"
      },
      "source": [
        "### Define the train loop\n",
        "\n",
        "We want the network to learn from every example in our training dataset. However, the best performance comes from more practice. Therefore, we **run through our dataset for multiple *epochs*.**\n",
        "\n",
        "After each epoch, we'll check how our model performs on the validation set to monitor its progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "pycharm": {},
        "id": "UOOVmj0dajLg"
      },
      "outputs": [],
      "source": [
        "from tqdm import tnrange, tqdm_notebook # import progress bars to show train progress\n",
        "\n",
        "def train_model(model, dataloaders, loss_function, optimizer, num_epochs):\n",
        "    \"\"\"\n",
        "    Trains a model using the given loss function and optimizer, for a certain number of epochs.\n",
        "\n",
        "    model: a PyTorch neural network\n",
        "    loss_function: a mathematical function that compares predictions and labels to return an error\n",
        "    num_epochs: the number of times to run through the full training dataset\n",
        "    \"\"\"\n",
        "    # train for n epochs. an epoch is a full iteration through our dataset\n",
        "    for epoch in tnrange(num_epochs, desc=\"Total progress\", unit=\"epoch\"):\n",
        "        # print a header\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('----------------')\n",
        "\n",
        "        # first train over the dataset and update weights; at the end, calculate our validation performance\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            # keep track of the overall loss and accuracy for this batch\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # iterate through the inputs and labels in our dataloader\n",
        "            # (the tqdm_notebook part is to display a progress bar)\n",
        "            for inputs, labels in tqdm_notebook(dataloaders[phase], desc=phase, unit=\"batch\", leave=False):\n",
        "                # move inputs and labels to appropriate device (GPU or CPU)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # FORWARD PASS\n",
        "                outputs = model(inputs)\n",
        "                # compute the error of the model's predictions\n",
        "                loss = loss_function(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    # BACKWARD PASS\n",
        "                    optimizer.zero_grad()  # clear the previous gradients\n",
        "                    loss.backward()        # backpropagate the current error gradients\n",
        "                    optimizer.step()       # update the weights (i.e. do the learning)\n",
        "\n",
        "                # track our accumulated loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # track number of correct to compute accuracy\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # print our progress\n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        "            print(f'{phase} error: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "QTKxQQ3VajLh"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "One last thing: we must define a function that gives feedback for how well the model performs. This is the **loss**, or \"error\" **function**, that compares model predictions to the true labels.\n",
        "\n",
        "Once we calculate the error, we also need to define how the model should react to that feedback. **The optimizer determines how the network learns from feedback.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "ROoXhvhyajLh"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss()              # the most common error function in deep learning\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent, with a learning rate of 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbNVYdRajLl"
      },
      "source": [
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/gradient_descent.gif?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "y5y3FO1uajLl"
      },
      "source": [
        "### Run training\n",
        "\n",
        "Let's put everything together and TRAIN OUR MODEL! =D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "pycharm": {},
        "id": "OTHsQYqJajLm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, dataloaders, loss_function, optimizer, num_epochs=3):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        for images, labels in dataloaders['train']:\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = loss_function(outputs, labels)  # Calculate loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Now you can call the train_model function\n",
        "train_model(model, dataloaders, loss_function, optimizer, num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "zZpaVjyuajLm"
      },
      "source": [
        "## Examine model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4MsllrNajLm"
      },
      "source": [
        "<img src=\"https://github.com/patitimoner/workshop-chihuahua-vs-muffin/blob/master/resources/question_mark.jpg?raw=1\" width=\"200\" >\n",
        "\n",
        "**How do we examine our model's predictions? Let's visualize what the model thinks on the validation set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "pycharm": {},
        "id": "Q_jyHjx0ajLm"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    pred_logits_tensor = model(validation_batch)\n",
        "    pred_probs = torch.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()  # Get probabilities\n",
        "\n",
        "# Show the probabilities for each picture\n",
        "fig, axs = plt.subplots(6, 5, figsize=(20, 20))\n",
        "for i, img_path in enumerate(validation_img_paths):\n",
        "    if i >= 30:  # Limit to 30 images\n",
        "        break\n",
        "    ax = axs[floor(i / 5)][i % 5]\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"{:.0f}% Chi, {:.0f}% Muff\".format(100 * pred_probs[i, 0], 100 * pred_probs[i, 1]), fontsize=18)\n",
        "    ax.imshow(Image.open(img_path))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uVmXm1WajLm"
      },
      "source": [
        "**Consider:** How accurate was your model? How confident were its predictions? Does it make clear-cut decisions?\n",
        "\n",
        "## Congratulations! You've successfully trained a neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-GaafjajLn"
      },
      "source": [
        "# III.  Can You Do Better?\n",
        "\n",
        "Now that we've shown you how to train a neural network, can you improve the validation accuracy by tweaking the parameters? **We challenge you to reach 100% accuracy!**\n",
        "\n",
        "Some parameters to play with:\n",
        "- Number of epochs\n",
        "- The learning rate \"lr\" parameter in the optimizer\n",
        "- The type of optimizer (https://pytorch.org/docs/stable/optim.html)\n",
        "- Number of layers and layer dimensions\n",
        "- Image size\n",
        "- Data augmentation transforms (https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "EzJH1wFfajLn"
      },
      "source": [
        "# Special Thanks!\n",
        "\n",
        "Credit for the original idea and code goes to [DeepSense.ai](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/)!\n",
        "We've modified it significantly to cater to this workshop, and boost the visual appeal.\n",
        "\n",
        "This tutorial was created through hard work and love by Jing Zhao, Dylan Wang, Jason Do, Jason Jiang, and Andrew Jong."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MkxiJiliuWA"
      },
      "execution_count": 153,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}